behaviors:
  TeenAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.01
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 4
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      curiosity:
        gamma: 0.99
        strength: 0.03
        encoding_size: 512
        learning_rate: 0.0003
    behavioral_cloning:
      demo_path: ./demos/TeenAgent_demos.demo
      strength: 0.5
      steps: 100000
    keep_checkpoints: 10
    max_steps: 10000000
    time_horizon: 256
    summary_freq: 5000
    checkpoint_interval: 100000
    threaded: true
    self_play:
      save_steps: 50000
      team_change: 200000
      swap_steps: 50000
      window: 10
      play_against_latest_model_ratio: 0.5

# Advanced Configuration for Better Performance:
#
# This config is for more sophisticated training with:
# - Larger neural network (512 units, 4 layers)
# - Behavioral cloning support (learn from demonstrations)
# - Self-play capabilities (teen learns from past versions)
# - Higher curiosity for exploring emotional responses
#
# Use this after initial training or when you have demonstration data
#
# To use behavioral cloning:
# 1. Record demonstrations using the Demonstration Recorder
# 2. Place .demo files in ./demos/ folder
# 3. Teen will initially imitate realistic behaviors
# 4. Then improve through reinforcement learning

