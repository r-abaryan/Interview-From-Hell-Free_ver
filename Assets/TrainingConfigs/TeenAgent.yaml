behaviors:
  TeenAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      curiosity:
        gamma: 0.99
        strength: 0.02
        encoding_size: 256
        learning_rate: 0.0003
    keep_checkpoints: 1
    max_steps: 5000000
    time_horizon: 128
    summary_freq: 10000
    checkpoint_interval: 1000000
    threaded: true

# Configuration Details:
# 
# trainer_type: ppo
#   - Using Proximal Policy Optimization, best for continuous learning scenarios
#
# batch_size: 1024
#   - Number of experiences per gradient descent update
#   - Larger batch = more stable but slower learning
#
# buffer_size: 10240
#   - Number of experiences to collect before updating
#   - Should be multiple of batch_size
#
# learning_rate: 0.0003
#   - How quickly the agent learns
#   - 0.0003 is a good starting point for PPO
#
# beta: 0.005
#   - Entropy regularization coefficient
#   - Encourages exploration of different responses
#
# epsilon: 0.2
#   - PPO clipping parameter
#   - Prevents too-large policy updates
#
# hidden_units: 256
#   - Size of neural network hidden layers
#   - Larger = more capacity to learn complex behaviors
#
# num_layers: 3
#   - Number of hidden layers in neural network
#   - 3 layers for moderately complex behavior
#
# curiosity reward:
#   - Encourages teen to explore different response strategies
#   - Helps agent discover realistic emotional responses
#
# max_steps: 5000000
#   - Total training steps (5 million)
#   - Adjust based on convergence
#
# time_horizon: 128
#   - How far ahead agent considers consequences
#   - Important for multi-turn conversations

